{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Yin Wen Tsai (蔡尹文)\n",
    "\n",
    "Student ID: 109072111\n",
    "\n",
    "GitHub ID: Ella0921"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data). The dataset contains a `sentiment` and `comment` columns, with the sentiment labels being: 'nostalgia' and 'not nostalgia'. Read the specificiations of the dataset for background details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/didiersalazar/DM2024-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Take home exercise of lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "\n",
    "#Answer here\n",
    "\n",
    "# Filter rows where the 'text' column contains the word 'finance' (case insensitive)\n",
    "finance_rows = X[X['text'].str.contains('finance', case=False)]\n",
    "\n",
    "# Display the result\n",
    "print(finance_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Replace string versions of missing values with np.nan\n",
    "NA_df['missing_example'] = NA_df['missing_example'].replace(['NaN', 'None', ''], np.nan)\n",
    "\n",
    "# Now apply isnull() to check for missing values again\n",
    "missing_values_check = NA_df['missing_example'].isnull()\n",
    "\n",
    "# Display the result\n",
    "print(missing_values_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# 1. Compare the number of rows in X and X_sample\n",
    "print(f\"Original X has {X.shape[0]} rows, while X_sample has {X_sample.shape[0]} rows.\")\n",
    "\n",
    "# 2. Check if any rows in X_sample are duplicated in the sample (which would be unlikely with n=1000, but possible)\n",
    "print(\"Are there any duplicates in the sampled dataframe?\")\n",
    "print(X_sample.duplicated().sum())\n",
    "\n",
    "# 3. Check the indices in X_sample to ensure they match those in the original X (they should reflect the original indices)\n",
    "print(\"Check the indices in X_sample:\")\n",
    "print(X_sample.index)\n",
    "\n",
    "# 4. Check if X_sample has all the columns of X and if the structure remains the same\n",
    "print(\"Do X and X_sample have the same columns?\")\n",
    "print(X.columns.equals(X_sample.columns))\n",
    "\n",
    "# 5. Verify that the values in X_sample match the corresponding rows in X by comparing a subset\n",
    "print(\"Check if the values in X_sample exist in X:\")\n",
    "print(X_sample.equals(X.loc[X_sample.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Get the value counts for both X and X_sample\n",
    "X_counts = X['category_name'].value_counts()\n",
    "X_sample_counts = X_sample['category_name'].value_counts()\n",
    "\n",
    "# Align categories (handle missing categories by filling with 0)\n",
    "categories = X_counts.index.union(X_sample_counts.index)\n",
    "X_counts = X_counts.reindex(categories, fill_value=0)\n",
    "X_sample_counts = X_sample_counts.reindex(categories, fill_value=0)\n",
    "\n",
    "# Set the upper bound for Y-axis\n",
    "upper_bound = max(X_counts.max(), X_sample_counts.max()) + 50\n",
    "\n",
    "# Plot both value counts side by side\n",
    "X_counts.plot(kind='bar', width=0.2, position=1, \n",
    "                   label='X dataset', ylim=[0, upper_bound], \n",
    "                   rot=0, fontsize=12, figsize=(8, 3))\n",
    "X_sample_counts.plot(kind='bar', width=0.2, position=0, \n",
    "                     label='X_sample dataset', ylim=[0, upper_bound], \n",
    "                     rot=0, fontsize=12, figsize=(8, 3), color='orange')\n",
    "\n",
    "# Add title and legend\n",
    "plt.title('Category distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Get the feature names (vocabulary terms)\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "\n",
    "# Get the fifth record (the 5th row from the sparse matrix, zero-indexed)\n",
    "fifth_record = X_counts[4]\n",
    "\n",
    "# Convert the sparse row to a dense array\n",
    "fifth_record_dense = fifth_record.toarray()\n",
    "\n",
    "# Find the indices where the value is 1 (which represents the presence of a term)\n",
    "indices = fifth_record_dense[0].nonzero()[0]\n",
    "\n",
    "# Map the indices back to the corresponding words\n",
    "second_one_index = indices[1]  # the second \"1\"\n",
    "second_word = feature_names[second_one_index]\n",
    "print(f\"The word of the second '1' in the fifth record is: {second_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 11\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Threshold: Only include terms that appear in at least 3 documents\n",
    "term_threshold = 3\n",
    "document_threshold = 3\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Transform the term-document matrix to TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(X_counts).toarray()\n",
    "\n",
    "# Sum across rows (documents) and columns (terms) to identify those with enough occurrences\n",
    "non_zero_rows = np.where((tfidf_matrix > 0).sum(axis=1) >= document_threshold)[0]\n",
    "non_zero_cols = np.where((tfidf_matrix > 0).sum(axis=0) >= term_threshold)[0]\n",
    "\n",
    "# Subset the matrix after applying the threshold\n",
    "filtered_matrix = tfidf_matrix[non_zero_rows][:, non_zero_cols]\n",
    "\n",
    "# Get the corresponding document and term labels after filtering\n",
    "filtered_plot_x = [\"term_\" + str(i) for i in np.array(count_vect.get_feature_names_out())[non_zero_cols]]\n",
    "filtered_plot_y = [\"doc_\" + str(i) for i in np.array(X.index)[non_zero_rows]]\n",
    "\n",
    "# Randomly sample 50 documents and 50 terms\n",
    "sample_size = 50\n",
    "sampled_rows = np.random.choice(filtered_matrix.shape[0], size=min(sample_size, filtered_matrix.shape[0]), replace=False)\n",
    "sampled_cols = np.random.choice(filtered_matrix.shape[1], size=min(sample_size, filtered_matrix.shape[1]), replace=False)\n",
    "\n",
    "# Subset the filtered matrix with the sampled rows and columns\n",
    "sampled_matrix = filtered_matrix[sampled_rows][:, sampled_cols]\n",
    "\n",
    "# Get the corresponding labels for the sampled rows and columns\n",
    "sampled_plot_x = [filtered_plot_x[i] for i in sampled_cols]\n",
    "sampled_plot_y = [filtered_plot_y[i] for i in sampled_rows]\n",
    "\n",
    "# Create DataFrame for the sampled matrix and plot it\n",
    "df_sampled_todraw = pd.DataFrame(sampled_matrix, columns=sampled_plot_x, index=sampled_plot_y)\n",
    "\n",
    "# Plot the sampled matrix\n",
    "plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(df_sampled_todraw, cmap=\"PuRd\", vmin=0, vmax=1, annot=False)\n",
    "plt.title('Sampled Term-Document Matrix (50 Documents and 50 Terms)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 12\n",
    "\n",
    "# Answer here\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a DataFrame for the plot\n",
    "df_term_frequencies = pd.DataFrame({\n",
    "    'terms': count_vect.get_feature_names_out()[:300],  # Adjust the number of terms as needed\n",
    "    'frequency': term_frequencies[:300]\n",
    "})\n",
    "\n",
    "# Create an interactive bar chart using Plotly\n",
    "fig = px.bar(df_term_frequencies, x='terms', y='frequency', title='Term Frequencies',\n",
    "             labels={'terms': 'Terms', 'frequency': 'Frequency'})\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-90,  # Rotate x-axis labels\n",
    "    xaxis_title=\"Terms\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    width=1200,  # Adjust the width of the figure\n",
    "    height=600   # Adjust the height of the figure\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 13\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Compute the total frequency of each term across all documents\n",
    "term_frequencies = np.asarray(X_counts.sum(axis=0))[0]\n",
    "\n",
    "# Create a DataFrame for terms and their frequencies\n",
    "df_term_frequencies = pd.DataFrame({\n",
    "    'terms': count_vect.get_feature_names_out(),\n",
    "    'frequency': term_frequencies\n",
    "})\n",
    "\n",
    "# Sort the terms by frequency and select the top K terms (e.g., 50)\n",
    "top_k = 50\n",
    "df_top_k = df_term_frequencies.sort_values(by='frequency', ascending=False).head(top_k)\n",
    "\n",
    "# Create an interactive Plotly bar chart for the top K terms\n",
    "fig = px.bar(df_top_k, x='terms', y='frequency', title=f'Top {top_k} Most Frequent Terms',\n",
    "             labels={'terms': 'Terms', 'frequency': 'Frequency'})\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-90,  # Rotate x-axis labels for readability\n",
    "    xaxis_title=\"Terms\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    width=1000,  # Adjust width of the plot\n",
    "    height=600   # Adjust height of the plot\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 14\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Compute the total frequency of each term across all documents\n",
    "term_frequencies = np.asarray(X_counts.sum(axis=0))[0]\n",
    "\n",
    "# Create a DataFrame for terms and their frequencies\n",
    "df_term_frequencies = pd.DataFrame({\n",
    "    'terms': count_vect.get_feature_names_out(),\n",
    "    'frequency': term_frequencies\n",
    "})\n",
    "\n",
    "# Sort the terms by frequency and select the top K terms\n",
    "top_k = 150\n",
    "df_top_k = df_term_frequencies.sort_values(by='frequency', ascending=False).head(top_k)\n",
    "\n",
    "# Create an interactive Plotly bar chart for the top K terms\n",
    "fig = px.bar(df_top_k, x='terms', y='frequency', title=f'Top {top_k} Most Frequent Terms',\n",
    "             labels={'terms': 'Terms', 'frequency': 'Frequency'})\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-90,  # Rotate x-axis labels for readability\n",
    "    xaxis_title=\"Terms\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    width=1000,  # Adjust width of the plot\n",
    "    height=600   # Adjust height of the plot\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 15\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Compute the total frequency of each term across all documents\n",
    "term_frequencies_log = [math.log(i) for i in term_frequencies]\n",
    "\n",
    "# Create a DataFrame for terms and their frequencies\n",
    "df_term_frequencies_log = pd.DataFrame({\n",
    "    'terms': count_vect.get_feature_names_out(),\n",
    "    'frequency': term_frequencies_log\n",
    "})\n",
    "\n",
    "# Sort the terms by frequency and select the top K terms\n",
    "top_k = 150\n",
    "df_top_k = df_term_frequencies_log.sort_values(by='frequency', ascending=False).head(top_k)\n",
    "\n",
    "# Create an interactive Plotly bar chart for the top K terms\n",
    "fig = px.bar(df_top_k, x='terms', y='frequency', title=f'Top {top_k} Most Frequent Terms',\n",
    "             labels={'terms': 'Terms', 'frequency': 'Frequency'})\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-90,  # Rotate x-axis labels for readability\n",
    "    xaxis_title=\"Terms\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    width=1000,  # Adjust width of the plot\n",
    "    height=600   # Adjust height of the plot\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# The difference between two plots\n",
    "\n",
    "'''The log transformation compresses the high-frequency terms, making the long tail of rare terms more visible.In the pre-log chart, common terms dominate, and rare terms are compressed near the axis. After the log transformation, the y-axis range is smoother, allowing for better comparison of both common and rare terms. This transformation is useful for handling skewed data, making infrequent but significant terms easier to observe.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 16\n",
    "\n",
    "# Answer here\n",
    "\n",
    "#Differences:\n",
    "\n",
    "## Domain-Specific Words:\n",
    "'''Categories have unique top 5% terms based on the subject matter, like \"distributed\" in comp.graphics, \"pagan\" in soc.religion.christian, and \"genetic\" in sci.med.'''\n",
    "## Obscure/Unique Words in Bottom 1%:\n",
    "'''Bottom 1% terms are often rare, technical, or misspelled (e.g., \"initworld\" in comp.graphics, \"remarriage\" in soc.religion.christian, and \"zyklon\" in alt.atheism).'''\n",
    "\n",
    "#Similarities:\n",
    "\n",
    "## Common Stop Words:\n",
    "'''Words like \"the,\" \"and,\" and \"to\" appear in the top 5% across all categories.'''\n",
    "## Rare and Specialized Terms:\n",
    "'''Bottom 1% words are highly specific, often used in only one or a few documents.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 17\n",
    "\n",
    "from PAMI.frequentPattern.topk import FAE as fae_alg\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Define k values for FAE Top-K trials\n",
    "k_values = [500, 1000, 1500]\n",
    "\n",
    "# Run FAE Top-K for each k value and save the patterns\n",
    "for k in k_values:\n",
    "    obj = fae_alg.FAE(iFile='td_freq_db_comp_graphics.csv', k=k)\n",
    "    obj.mine()  # Run the mining process\n",
    "    \n",
    "    # Get patterns\n",
    "    fae_patterns = obj.getPatterns()\n",
    "    print(f\"FAE Top-K with k={k}:\")\n",
    "    print('Total No of patterns: ' + str(len(fae_patterns)))\n",
    "    print('Runtime: ' + str(obj.getRuntime()) +'\\n')\n",
    "    # Save patterns to file\n",
    "    obj.save(f'patterns_faetopk_k{k}.txt')\n",
    "    \n",
    "\n",
    "from PAMI.frequentPattern.maximal import MaxFPGrowth as alg\n",
    "\n",
    "# Define the minimum support values for the algorithm\n",
    "support_values = [3, 6, 9]\n",
    "\n",
    "# Loop through each support value, mine the patterns, and save the results\n",
    "for support in support_values:\n",
    "    obj = alg.MaxFPGrowth(\"td_freq_db_sci_med.csv\", support)  # Initialize algorithm\n",
    "    obj.mine()  # Run the mining process\n",
    "    \n",
    "    # Get patterns as DataFrame\n",
    "    patterns_df = obj.getPatterns()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"MaxFPGrowth with minimum support thresholds={support}:\")\n",
    "    print('Total No of patterns: ' + str(len(patterns_df)))\n",
    "    print('Runtime: ' + str(obj.getRuntime())+'\\n')\n",
    "    \n",
    "    # Save patterns to file\n",
    "    obj.save(f'patterns_maxfp_minsup{support}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 18\n",
    "\n",
    "# Answer here\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Apply dimensionality reduction to 3 components\n",
    "X_pca_3d = PCA(n_components=3).fit_transform(tdm_df.values)\n",
    "X_tsne_3d = TSNE(n_components=3).fit_transform(tdm_df.values)\n",
    "X_umap_3d = umap.UMAP(n_components=3).fit_transform(tdm_df.values)\n",
    "\n",
    "\n",
    "# Define the colors for the categories\n",
    "col = ['coral', 'blue', 'black', 'orange']\n",
    "categories = X['category_name'].unique()\n",
    "\n",
    "# Define a function to plot 3D scatter with different angles\n",
    "def plot_3d_scatter(ax, X_reduced, title, elev=30, azim=45):\n",
    "    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=X['category_name'].map(dict(zip(categories, col))), marker='o')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "# Create 3D plots with different algorithms and camera angles\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot PCA results from different angles\n",
    "ax1 = fig.add_subplot(231, projection='3d')\n",
    "plot_3d_scatter(ax1, X_pca_3d, 'PCA - Angle 1', elev=30, azim=45)\n",
    "\n",
    "ax2 = fig.add_subplot(232, projection='3d')\n",
    "plot_3d_scatter(ax2, X_pca_3d, 'PCA - Angle 2', elev=60, azim=120)\n",
    "\n",
    "ax3 = fig.add_subplot(233, projection='3d')\n",
    "plot_3d_scatter(ax3, X_pca_3d, 'PCA - Angle 3', elev=90, azim=180)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "# Create 3D plots with different algorithms and camera angles\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot t-SNE results from different angles\n",
    "ax4 = fig.add_subplot(234, projection='3d')\n",
    "plot_3d_scatter(ax4, X_tsne_3d, 't-SNE - Angle 1', elev=30, azim=45)\n",
    "\n",
    "ax5 = fig.add_subplot(235, projection='3d')\n",
    "plot_3d_scatter(ax5, X_tsne_3d, 't-SNE - Angle 2', elev=60, azim=120)\n",
    "\n",
    "ax6 = fig.add_subplot(236, projection='3d')\n",
    "plot_3d_scatter(ax6, X_tsne_3d, 't-SNE - Angle 3', elev=90, azim=180)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "# Create 3D plots with different algorithms and camera angles\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot UMAP results from different angles\n",
    "ax7 = fig.add_subplot(337, projection='3d')\n",
    "plot_3d_scatter(ax7, X_umap_3d, 'UMAP - Angle 1', elev=30, azim=45)\n",
    "\n",
    "ax8 = fig.add_subplot(338, projection='3d')\n",
    "plot_3d_scatter(ax8, X_umap_3d, 'UMAP - Angle 2', elev=60, azim=120)\n",
    "\n",
    "ax9 = fig.add_subplot(339, projection='3d')\n",
    "plot_3d_scatter(ax9, X_umap_3d, 'UMAP - Angle 3', elev=90, azim=180)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 19\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Initialize the LabelBinarizer\n",
    "mlb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Fit the binarizer to the category_name column\n",
    "mlb.fit(X['category_name'])\n",
    "\n",
    "# Transform the category_name column into binary attributes\n",
    "X['bin_category_name'] = mlb.transform(X['category_name']).tolist()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(X[['category_name', 'bin_category_name']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attempt the tasks on the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
